{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _resume_eval_import_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tiktoken \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from prompts.resume_eval import RESUME_EVALUATION_PROMPT\n",
    "\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime \n",
    "from typing import Dict, Any, Union\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv(\"../../.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating the cost \n",
    "def count_tokens(input_string: str) -> int:\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = tokenizer.encode(input_string)\n",
    "    return len(tokens)\n",
    "\n",
    "def calculate_cost(input_string: str, cost_per_million_tokens: float=5) -> float:\n",
    "    num_tokens = count_tokens(input_string)\n",
    "    total_cost = (num_tokens/1_000_000) * cost_per_million_tokens\n",
    "    return total_cost\n",
    "\n",
    "# import anthropic\n",
    "\n",
    "# client = anthropic.Client()\n",
    "# token_count = client.count_tokens(complete_template)\n",
    "# print(token_count)\n",
    "# generate unique id\n",
    "def generate_unique_id(_):\n",
    "    return str(uuid4())\n",
    "\n",
    "# compare model outputs\n",
    "def compare_model_outputs(model_results):\n",
    "    \"\"\"\n",
    "    Compare original and recalibrated scores from different models and output a pretty table.\n",
    "    \n",
    "    :param model_results: Dict with model names as keys and JSON outputs as values\n",
    "    :return: pandas DataFrame with a pretty table comparison\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    score_types = [\n",
    "        \"technical_skills\",\n",
    "        \"soft_skills\",\n",
    "        \"required_experience\",\n",
    "        \"qualifications\"\n",
    "    ]\n",
    "    \n",
    "    for model, result in model_results.items():\n",
    "        # Ensure result is a dictionary\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "        \n",
    "        model_data = {\"Model\": model}\n",
    "        \n",
    "        # Extract original scores\n",
    "        original_scores = result.get(\"resume_evaluation\", {}).get(\"original_scores\", {})\n",
    "        \n",
    "        # Extract recalibrated scores\n",
    "        recalibrated_scores = result.get(\"recalibrated_scores\", {})\n",
    "        \n",
    "        for score_type in score_types:\n",
    "            model_data[f\"original_{score_type}\"] = original_scores.get(score_type, \"N/A\")\n",
    "            model_data[f\"recalibrated_{score_type}\"] = recalibrated_scores.get(score_type, \"N/A\")\n",
    "        \n",
    "        # Add suitability\n",
    "        model_data[\"suitability\"] = result.get(\"assessment\", {}).get(\"suitability\", \"N/A\")\n",
    "        \n",
    "        comparison_data.append(model_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\"Model\"] + [f\"{prefix}_{score_type}\" for score_type in score_types for prefix in [\"original\", \"recalibrated\"]] + [\"suitability\"]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# evaluate resume \n",
    "def evaluate_resume(job_description: str, resume: str, job_id: str, cv_id: str, output_dir: str = \"./output/\") -> Union[pd.DataFrame, None]:\n",
    "    model_results = {}\n",
    "    \n",
    "    for model_name, grader in [(\"llama3\", llama3_grader), (\"gpt\", gpt_grader), (\"anthropic\", anthropic_grader)]:\n",
    "        try:\n",
    "            result = grader.invoke({\"job_description\": job_description, \"resume\": resume})\n",
    "            model_results[model_name] = result\n",
    "\n",
    "            # save model result \n",
    "            json_file = os.path.join(output_dir, f\"{job_id}_{cv_id}_{model_name}.json\")\n",
    "            with open(json_file, \"w\") as f:\n",
    "                json.dump(result, f, indent=4)\n",
    "            \n",
    "            time.sleep(1)  # Add a small delay to avoid rate limiting\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with {model_name} for job_id: {job_id}, cv_id: {cv_id}. Error: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            print(error_msg)\n",
    "\n",
    "    if not model_results:\n",
    "        error_msg = f\"All models failed for job_id: {job_id}, cv_id: {cv_id}.\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        comparison_df = compare_model_outputs(model_results)\n",
    "        \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # save comparison result \n",
    "        csv_file = os.path.join(output_dir, f\"{job_id}_{cv_id}.csv\")\n",
    "        comparison_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "        logging.info(f\"Successfully processed and saved results for job_id: {job_id}, cv_id: {cv_id}.\")\n",
    "        \n",
    "        # return comparison_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in processing or saving results for job_id: {job_id}, cv_id: {cv_id}. Error: {error_msg}.\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current time \n",
    "current_time = datetime.now().strftime((\"%Y%m%d_%H%M\"))\n",
    "output_dir = f\"./output_{current_time}/\"\n",
    "\n",
    "# create output directory \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "temperature = 0\n",
    "max_tokens = 2048\n",
    "\n",
    "# set up logger \n",
    "log_file = os.path.join(output_dir, \"evaluation_log.txt\")\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO, filename=log_file, datefmt=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../repos/Resume-Screening-RAG-Pipeline/data/supplementary-data/\"\n",
    "synthetic_data_path = \"../repos/Resume-Screening-RAG-Pipeline/data/main-data/\"\n",
    "\n",
    "job_description_path = data_path + \"job_title_des.csv\"\n",
    "cleaned_resume_path = data_path + \"cleaned_resume.csv\"\n",
    "synthetic_resume_path = synthetic_data_path + \"synthetic-resumes.csv\"\n",
    "\n",
    "cleaned_resume = pd.read_csv(cleaned_resume_path)\n",
    "synthetic_resumes = pd.read_csv(synthetic_resume_path)\n",
    "job_description = pd.read_csv(job_description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_resume), len(synthetic_resumes), len(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_resume[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_resume.columns, synthetic_resumes.columns, job_description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply unique id \n",
    "job_description[\"Job ID\"] = job_description[\"Job Title\"].apply(generate_unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the resumes \n",
    "# we can sample the job pool to reduce the cost \n",
    "tech_jobs = ['Data Science', \n",
    "      'Java Developer', 'Business Analyst',\n",
    "      'SAP Developer','Python Developer', 'DevOps Engineer',\n",
    "      'Network Security Engineer','Database', 'Hadoop',\n",
    "      'ETL Developer', 'DotNet Developer', 'Blockchain']\n",
    "\n",
    "non_tech_jobs = [\n",
    "  'HR', 'Advocate', 'Arts', 'Web Designing', 'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
    "  'Civil Engineer',  'Automation Testing', 'Electrical Engineering',\n",
    "  'Operations Manager',  'PMO',\n",
    "]\n",
    "\n",
    "tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(tech_jobs)] # 88\n",
    "non_tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(non_tech_jobs)] # 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(tech_jobs)] # 88\n",
    "non_tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(non_tech_jobs)] # 67\n",
    "\n",
    "reduced_tech_pool = tech_pool.groupby(\"Category\").apply(lambda x: x.sample(frac=0.5, random_state=42)).reset_index(drop=True)\n",
    "reduced_non_tech_pool = non_tech_pool[non_tech_pool[\"Category\"].isin([\"HR\", \"Advocate\", \"Arts\"])].sample(3, random_state=42)\n",
    "\n",
    "talent_pool = pd.concat([reduced_tech_pool, reduced_non_tech_pool]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.DataFrame()\n",
    "\n",
    "for title in job_description[\"Job Title\"].unique():\n",
    "  jobs = pd.concat([jobs, job_description[job_description[\"Job Title\"]==title].sample(1, random_state=42)])\n",
    "\n",
    "jobs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description.to_csv(os.path.join(output_dir, \"job_description.csv\"), index=False)\n",
    "talent_pool.to_csv(os.path.join(output_dir, \"filtered_talent_pool.csv\"), index=False)\n",
    "jobs.to_csv(os.path.join(output_dir, \"filtered_job_description.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanitary check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jd = job_description[job_description[\"Job Title\"]==\"Machine Learning\"][\"Job Description\"].values[0]\n",
    "# jd_id = job_description[job_description[\"Job Title\"]==\"Machine Learning\"][\"Job ID\"].values[0]\n",
    "# cv = cleaned_resume.iloc[77][\"Resume\"]\n",
    "# cv_id = cleaned_resume.iloc[77][\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature = 0\n",
    "# max_tokens = 2048\n",
    "\n",
    "# # groq_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=temperature, max_tokens=max_tokens)\n",
    "# gpt_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)\n",
    "# anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=temperature, max_tokens=max_tokens)\n",
    "# llama3_llm = ChatOllama(model=\"llama3\", temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "# resume_eval_prompt = PromptTemplate(\n",
    "#     input_variables=[\"job_description\", \"resume\"],\n",
    "#     template=RESUME_EVALUATION_PROMPT\n",
    "#     )\n",
    "\n",
    "# # llama3\n",
    "# groq_grader = resume_eval_prompt | groq_llm | JsonOutputParser()\n",
    "# groq_result = groq_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# # gpt\n",
    "# gpt_grader = resume_eval_prompt | gpt_llm | JsonOutputParser()\n",
    "# gpt_result = gpt_grader.invoke({\"job_description\": jd, \"resume\": cv}) \n",
    "\n",
    "# # claude\n",
    "# anthropic_grader = resume_eval_prompt | anthropic_llm | JsonOutputParser()\n",
    "# anthropic_result = anthropic_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# llama3_grader = resume_eval_prompt | llama3_llm | JsonOutputParser()\n",
    "# llama3_result = llama3_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# model_results = {\n",
    "#     \"groq\": groq_result,\n",
    "#     \"anthropic\": anthropic_result,\n",
    "#     \"gpt\": gpt_result\n",
    "# }\n",
    "\n",
    "# comparison_df = compare_model_outputs(model_results)\n",
    "\n",
    "# # Display the pretty table\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# print(comparison_df.to_string(index=False))\n",
    "\n",
    "# # If you want to save it to a CSV file:\n",
    "# # comparison_df.to_csv(\"model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=temperature, max_tokens=max_tokens)\n",
    "gpt_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)\n",
    "anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=temperature, max_tokens=max_tokens)\n",
    "llama3_llm = ChatOllama(model=\"llama3\", temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "resume_eval_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\", \"resume\"],\n",
    "    template=RESUME_EVALUATION_PROMPT\n",
    "    )\n",
    "\n",
    "gpt_grader = resume_eval_prompt | gpt_llm | JsonOutputParser()\n",
    "# groq_grader = resume_eval_prompt | groq_llm | JsonOutputParser()\n",
    "anthropic_grader = resume_eval_prompt | anthropic_llm | JsonOutputParser()\n",
    "llama3_grader = resume_eval_prompt | llama3_llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_cv_pair(jod_data, cv_data):\n",
    "    job_id, job_description = jod_data\n",
    "    cv_id, cv = cv_data\n",
    "    return evaluate_resume(job_description, cv, job_id, cv_id)\n",
    "\n",
    "def process_all_pairs():\n",
    "    job_data = jobs[[\"Job ID\", \"Job Description\"]].values\n",
    "    cv_data = talent_pool[[\"ID\", \"Resume\"]].values\n",
    "\n",
    "    total_pairs = len(job_data) * len(cv_data)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = [] \n",
    "        for job in job_data:\n",
    "            for cv in cv_data:\n",
    "                futures.append(executor.submit(process_job_cv_pair, job, cv))\n",
    "            \n",
    "        for future in tqdm(as_completed(futures), total=total_pairs, desc=\"Processing job-cv pairs\"):\n",
    "            try:\n",
    "                future = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_all_pairs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
