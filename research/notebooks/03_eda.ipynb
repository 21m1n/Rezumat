{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _resume_eval_import_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import tiktoken \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from uuid import uuid4\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI \n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from prompts.resume_eval import RESUME_EVALUATION_PROMPT\n",
    "from langchain_core.runnables.base import RunnableSequence\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime \n",
    "from typing import Dict, Any, Union, List, Tuple\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(find_dotenv(\"../../.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate unique id\n",
    "def generate_unique_id(_):\n",
    "    return str(uuid4())\n",
    "\n",
    "# compare model outputs\n",
    "def compare_model_outputs(model_results):\n",
    "    \"\"\"\n",
    "    Compare original and recalibrated scores from different models and output a pretty table.\n",
    "    \n",
    "    :param model_results: Dict with model names as keys and JSON outputs as values\n",
    "    :return: pandas DataFrame with a pretty table comparison\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    score_types = [\n",
    "        \"technical_skills\",\n",
    "        \"soft_skills\",\n",
    "        \"required_experience\",\n",
    "        \"qualifications\"\n",
    "    ]\n",
    "    \n",
    "    for model, result in model_results.items():\n",
    "        # Ensure result is a dictionary\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "        \n",
    "        model_data = {\"Model\": model}\n",
    "        \n",
    "        # Extract original scores\n",
    "        original_scores = result.get(\"resume_evaluation\", {}).get(\"original_scores\", {})\n",
    "        \n",
    "        # Extract recalibrated scores\n",
    "        recalibrated_scores = result.get(\"recalibrated_scores\", {})\n",
    "        \n",
    "        for score_type in score_types:\n",
    "            model_data[f\"original_{score_type}\"] = original_scores.get(score_type, \"N/A\")\n",
    "            model_data[f\"recalibrated_{score_type}\"] = recalibrated_scores.get(score_type, \"N/A\")\n",
    "        \n",
    "        # Add suitability\n",
    "        model_data[\"suitability\"] = result.get(\"assessment\", {}).get(\"suitability\", \"N/A\")\n",
    "        \n",
    "        comparison_data.append(model_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\"Model\"] + [f\"{prefix}_{score_type}\" for score_type in score_types for prefix in [\"original\", \"recalibrated\"]] + [\"suitability\"]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# evaluate resume \n",
    "def evaluate_resume(model_tuples: List[Tuple[str, RunnableSequence]], job_description: str, resume: str, job_id: str, cv_id: str, output_dir: str) -> Union[pd.DataFrame, None]:\n",
    "    model_results = {}\n",
    "    for model_name, grader in model_tuples:\n",
    "    # for model_name, grader in [(\"llama3\", llama3_grader), (\"gpt\", gpt_grader), (\"anthropic\", anthropic_grader)]:\n",
    "        try:\n",
    "            result = grader.invoke({\"job_description\": job_description, \"resume\": resume})\n",
    "            model_results[model_name] = result\n",
    "\n",
    "            # save model result \n",
    "            json_file = os.path.join(output_dir, f\"{job_id}_{cv_id}_{model_name}.json\")\n",
    "            with open(json_file, \"w\") as f:\n",
    "                json.dump(result, f, indent=4)\n",
    "            \n",
    "            time.sleep(2.1)  # Add a small delay to avoid rate limiting\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with {model_name} for job_id: {job_id}, cv_id: {cv_id}. Error: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            print(error_msg)\n",
    "\n",
    "    if not model_results:\n",
    "        error_msg = f\"All models failed for job_id: {job_id}, cv_id: {cv_id}.\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return None\n",
    "    \n",
    "    # try:\n",
    "        \n",
    "    #     comparison_df = compare_model_outputs(model_results)\n",
    "        \n",
    "    #     if not os.path.exists(output_dir):\n",
    "    #         os.makedirs(output_dir)\n",
    "        \n",
    "    #     # save comparison result \n",
    "    #     csv_file = os.path.join(output_dir, f\"{job_id}_{cv_id}.csv\")\n",
    "    #     comparison_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "    #     logging.info(f\"Successfully processed and saved results for job_id: {job_id}, cv_id: {cv_id}.\")\n",
    "        \n",
    "    #     # return comparison_df\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     error_msg = f\"Error in processing or saving results for job_id: {job_id}, cv_id: {cv_id}. Error: {error_msg}.\"\n",
    "    #     logging.error(error_msg)\n",
    "    #     print(error_msg)\n",
    "    #     return None\n",
    "\n",
    "def generate_unique_id(_):\n",
    "    return str(uuid4())\n",
    "\n",
    "# compare model outputs\n",
    "def compare_model_outputs(model_results):\n",
    "    \"\"\"\n",
    "    Compare original and recalibrated scores from different models and output a pretty table.\n",
    "    \n",
    "    :param model_results: Dict with model names as keys and JSON outputs as values\n",
    "    :return: pandas DataFrame with a pretty table comparison\n",
    "    \"\"\"\n",
    "    comparison_data = []\n",
    "    \n",
    "    score_types = [\n",
    "        \"technical_skills\",\n",
    "        \"soft_skills\",\n",
    "        \"required_experience\",\n",
    "        \"qualifications\"\n",
    "    ]\n",
    "    \n",
    "    for model, result in model_results.items():\n",
    "        # Ensure result is a dictionary\n",
    "        if isinstance(result, str):\n",
    "            result = json.loads(result)\n",
    "        \n",
    "        model_data = {\"Model\": model}\n",
    "        \n",
    "        # Extract original scores\n",
    "        original_scores = result.get(\"resume_evaluation\", {}).get(\"original_scores\", {})\n",
    "        \n",
    "        # Extract recalibrated scores\n",
    "        recalibrated_scores = result.get(\"recalibrated_scores\", {})\n",
    "        \n",
    "        for score_type in score_types:\n",
    "            model_data[f\"original_{score_type}\"] = original_scores.get(score_type, \"N/A\")\n",
    "            model_data[f\"recalibrated_{score_type}\"] = recalibrated_scores.get(score_type, \"N/A\")\n",
    "        \n",
    "        # Add suitability\n",
    "        model_data[\"suitability\"] = result.get(\"assessment\", {}).get(\"suitability\", \"N/A\")\n",
    "        \n",
    "        comparison_data.append(model_data)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Reorder columns\n",
    "    column_order = [\"Model\"] + [f\"{prefix}_{score_type}\" for score_type in score_types for prefix in [\"original\", \"recalibrated\"]] + [\"suitability\"]\n",
    "    df = df[column_order]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# evaluate resume \n",
    "def evaluate_resume(model_tuples: List[Tuple[str, RunnableSequence]], job_description: str, resume: str, job_id: str, cv_id: str, output_dir: str) -> Union[pd.DataFrame, None]:\n",
    "    model_results = {}\n",
    "    for model_name, grader in model_tuples:\n",
    "    # for model_name, grader in [(\"llama3\", llama3_grader), (\"gpt\", gpt_grader), (\"anthropic\", anthropic_grader)]:\n",
    "        try:\n",
    "            result = grader.invoke({\"job_description\": job_description, \"resume\": resume})\n",
    "            model_results[model_name] = result\n",
    "\n",
    "            # save model result \n",
    "            json_file = os.path.join(output_dir, f\"{job_id}_{cv_id}_{model_name}.json\")\n",
    "            with open(json_file, \"w\") as f:\n",
    "                json.dump(result, f, indent=4)\n",
    "            \n",
    "            time.sleep(2.1)  # Add a small delay to avoid rate limiting\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with {model_name} for job_id: {job_id}, cv_id: {cv_id}. Error: {str(e)}\"\n",
    "            logging.error(error_msg)\n",
    "            print(error_msg)\n",
    "\n",
    "    if not model_results:\n",
    "        error_msg = f\"All models failed for job_id: {job_id}, cv_id: {cv_id}.\"\n",
    "        logging.error(error_msg)\n",
    "        print(error_msg)\n",
    "        return None\n",
    "    \n",
    "    # try:\n",
    "        \n",
    "    #     comparison_df = compare_model_outputs(model_results)\n",
    "        \n",
    "    #     if not os.path.exists(output_dir):\n",
    "    #         os.makedirs(output_dir)\n",
    "        \n",
    "    #     # save comparison result \n",
    "    #     csv_file = os.path.join(output_dir, f\"{job_id}_{cv_id}.csv\")\n",
    "    #     comparison_df.to_csv(csv_file, index=False)\n",
    "        \n",
    "    #     logging.info(f\"Successfully processed and saved results for job_id: {job_id}, cv_id: {cv_id}.\")\n",
    "        \n",
    "    #     # return comparison_df\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     error_msg = f\"Error in processing or saving results for job_id: {job_id}, cv_id: {cv_id}. Error: {error_msg}.\"\n",
    "    #     logging.error(error_msg)\n",
    "    #     print(error_msg)\n",
    "    #     return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current time \n",
    "current_time = datetime.now().strftime((\"%Y%m%d_%H%M\"))\n",
    "output_dir = f\"./output_{current_time}/\"\n",
    "\n",
    "# create output directory \n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "temperature = 0\n",
    "max_tokens = 2048\n",
    "\n",
    "# set up logger \n",
    "log_file = os.path.join(output_dir, \"evaluation_log.txt\")\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.INFO, filename=log_file, datefmt=\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../repos/Resume-Screening-RAG-Pipeline/data/supplementary-data/\"\n",
    "synthetic_data_path = \"../repos/Resume-Screening-RAG-Pipeline/data/main-data/\"\n",
    "\n",
    "job_description_path = data_path + \"job_title_des.csv\"\n",
    "cleaned_resume_path = data_path + \"cleaned_resume.csv\"\n",
    "synthetic_resume_path = synthetic_data_path + \"synthetic-resumes.csv\"\n",
    "\n",
    "cleaned_resume = pd.read_csv(cleaned_resume_path)\n",
    "synthetic_resumes = pd.read_csv(synthetic_resume_path)\n",
    "job_description = pd.read_csv(job_description_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162, 1000, 2277)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_resume), len(synthetic_resumes), len(job_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Category\n",
       "Java Developer               13\n",
       "Database                     11\n",
       "Data Science                 10\n",
       "Advocate                     10\n",
       "DotNet Developer              7\n",
       "Hadoop                        7\n",
       "DevOps Engineer               7\n",
       "Automation Testing            7\n",
       "Testing                       7\n",
       "Python Developer              6\n",
       "Arts                          6\n",
       "Health and fitness            6\n",
       "Civil Engineer                6\n",
       "HR                            6\n",
       "SAP Developer                 6\n",
       "Business Analyst              6\n",
       "Electrical Engineering        5\n",
       "Network Security Engineer     5\n",
       "Mechanical Engineer           5\n",
       "ETL Developer                 5\n",
       "Blockchain                    5\n",
       "Sales                         5\n",
       "Operations Manager            4\n",
       "Web Designing                 4\n",
       "PMO                           3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_resume[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Category', 'Resume', 'ID'], dtype='object'),\n",
       " Index(['ID', 'Resume'], dtype='object'),\n",
       " Index(['Job Title', 'Job Description'], dtype='object'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_resume.columns, synthetic_resumes.columns, job_description.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply unique id \n",
    "job_description[\"Job ID\"] = job_description[\"Job Title\"].apply(generate_unique_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize the resumes \n",
    "# we can sample the job pool to reduce the cost \n",
    "tech_jobs = ['Data Science', \n",
    "      'Java Developer', 'Business Analyst',\n",
    "      'SAP Developer','Python Developer', 'DevOps Engineer',\n",
    "      'Network Security Engineer','Database', 'Hadoop',\n",
    "      'ETL Developer', 'DotNet Developer', 'Blockchain']\n",
    "\n",
    "non_tech_jobs = [\n",
    "  'HR', 'Advocate', 'Arts', 'Web Designing', 'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
    "  'Civil Engineer',  'Automation Testing', 'Electrical Engineering',\n",
    "  'Operations Manager',  'PMO',\n",
    "]\n",
    "\n",
    "tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(tech_jobs)] # 88\n",
    "non_tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(non_tech_jobs)] # 67"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x3/z5c7sxc131d8t8hhqfw7x3780000gn/T/ipykernel_55387/1156190809.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  reduced_tech_pool = tech_pool.groupby(\"Category\").apply(lambda x: x.sample(frac=0.5, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(tech_jobs)] # 88\n",
    "non_tech_pool = cleaned_resume[cleaned_resume[\"Category\"].isin(non_tech_jobs)] # 67\n",
    "\n",
    "reduced_tech_pool = tech_pool.groupby(\"Category\").apply(lambda x: x.sample(frac=0.5, random_state=42)).reset_index(drop=True)\n",
    "reduced_non_tech_pool = non_tech_pool[non_tech_pool[\"Category\"].isin([\"HR\", \"Advocate\", \"Arts\"])].sample(3, random_state=42)\n",
    "\n",
    "talent_pool = pd.concat([reduced_tech_pool, reduced_non_tech_pool]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.DataFrame()\n",
    "\n",
    "for title in job_description[\"Job Title\"].unique():\n",
    "  jobs = pd.concat([jobs, job_description[job_description[\"Job Title\"]==title].sample(1, random_state=42)])\n",
    "\n",
    "jobs.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description.to_csv(os.path.join(output_dir, \"job_description.csv\"), index=False)\n",
    "talent_pool.to_csv(os.path.join(output_dir, \"filtered_talent_pool.csv\"), index=False)\n",
    "jobs.to_csv(os.path.join(output_dir, \"filtered_job_description.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the resume\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanitary check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jd = job_description[job_description[\"Job Title\"]==\"Machine Learning\"][\"Job Description\"].values[0]\n",
    "# jd_id = job_description[job_description[\"Job Title\"]==\"Machine Learning\"][\"Job ID\"].values[0]\n",
    "# cv = cleaned_resume.iloc[77][\"Resume\"]\n",
    "# cv_id = cleaned_resume.iloc[77][\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature = 0\n",
    "# max_tokens = 2048\n",
    "\n",
    "# groq_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=temperature, max_tokens=max_tokens)\n",
    "# gpt_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=temperature, max_tokens=max_tokens)\n",
    "# anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=temperature, max_tokens=max_tokens)\n",
    "# llama3_1_llm = ChatOllama(model=\"llama3.1:8b\", temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "# resume_eval_prompt = PromptTemplate(\n",
    "#     input_variables=[\"job_description\", \"resume\"],\n",
    "#     template=RESUME_EVALUATION_PROMPT\n",
    "#     )\n",
    "\n",
    "# # llama3\n",
    "# groq_grader = resume_eval_prompt | groq_llm | JsonOutputParser()\n",
    "# groq_result = groq_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# # gpt\n",
    "# gpt_grader = resume_eval_prompt | gpt_llm | JsonOutputParser()\n",
    "# gpt_result = gpt_grader.invoke({\"job_description\": jd, \"resume\": cv}) \n",
    "\n",
    "# # claude\n",
    "# anthropic_grader = resume_eval_prompt | anthropic_llm | JsonOutputParser()\n",
    "# anthropic_result = anthropic_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# llama3_grader = resume_eval_prompt | llama3_llm | JsonOutputParser()\n",
    "# llama3_result = llama3_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# llama3_1_grader = resume_eval_prompt | llama3_1_llm | JsonOutputParser()\n",
    "# llama3_1_result = llama3_1_grader.invoke({\"job_description\": jd, \"resume\": cv})\n",
    "\n",
    "# model_results = {\n",
    "    # \"groq\": groq_result,\n",
    "    # \"anthropic\": anthropic_result,\n",
    "    # \"gpt\": gpt_result,\n",
    "    # \"llama3_1\": llama3_1_result\n",
    "# }\n",
    "\n",
    "# comparison_df = compare_model_outputs(model_results)\n",
    "\n",
    "# # Display the pretty table\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# print(comparison_df.to_string(index=False))\n",
    "\n",
    "# save it to a CSV file:\n",
    "# comparison_df.to_csv(\"model_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groq_llm = ChatGroq(model=\"llama3-70b-8192\", temperature=temperature, max_tokens=max_tokens)\n",
    "gpt_llm = ChatOpenAI(model=\"chatgpt-4o-latest\", temperature=temperature, max_tokens=max_tokens)\n",
    "# anthropic_llm = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\", temperature=temperature, max_tokens=max_tokens)\n",
    "# llama3_llm = ChatOllama(model=\"llama3\", temperature=temperature, max_tokens=max_tokens)\n",
    " \n",
    "resume_eval_prompt = PromptTemplate(\n",
    "    input_variables=[\"job_description\", \"resume\"],\n",
    "    template=RESUME_EVALUATION_PROMPT\n",
    "    )\n",
    "\n",
    "gpt_grader = resume_eval_prompt | gpt_llm | JsonOutputParser()\n",
    "# groq_grader = resume_eval_prompt | groq_llm | JsonOutputParser()\n",
    "# anthropic_grader = resume_eval_prompt | anthropic_llm | JsonOutputParser()\n",
    "# llama3_grader = resume_eval_prompt | llama3_llm | JsonOutputParser()\n",
    "\n",
    "model_tuples = [\n",
    "    (\"gpt4o\", gpt_grader),\n",
    "    # (\"anthropic\", anthropic_grader),\n",
    "    # (\"llama3\", llama3_grader)\n",
    "    # (\"groq\", groq_grader)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = pd.read_csv(os.path.join(\"output\", \"filtered_job_description.csv\"))\n",
    "talent_pool = pd.read_csv(os.path.join(\"output\", \"filtered_talent_pool.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_cv_pair(jod_data, cv_data):\n",
    "    job_id, job_description = jod_data\n",
    "    cv_id, cv = cv_data\n",
    "    return evaluate_resume(model_tuples, job_description, cv, job_id, cv_id, output_dir)\n",
    "\n",
    "def process_all_pairs():\n",
    "    job_data = jobs[[\"Job ID\", \"Job Description\"]].values\n",
    "    cv_data = talent_pool[[\"ID\", \"Resume\"]].values\n",
    "\n",
    "    total_pairs = len(job_data) * len(cv_data)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        futures = [] \n",
    "        for job in job_data:\n",
    "            for cv in cv_data:\n",
    "                futures.append(executor.submit(process_job_cv_pair, job, cv))\n",
    "            \n",
    "        for future in tqdm(as_completed(futures), total=total_pairs, desc=\"Processing job-cv pairs\"):\n",
    "            try:\n",
    "                future = future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbad48887804b30b6348ab808e25918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing job-cv pairs:   0%|          | 0/705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with gpt4o for job_id: ca0539ba-835b-48fb-bed0-705d6668c372, cv_id: 508190c8-d2e1-4d77-b19d-11a1aaa07ebd. Error: Invalid json output: ```json\n",
      "{\n",
      "  \"job_description_analysis\": {\n",
      "    \"technical_skills\": {\n",
      "      \"essential\": [\"ASP.NET\", \"SQL\", \"VBA\", \"VB.NET\", \"C#\"],\n",
      "      \"advantageous\": [\"SSRS\", \"ERP systems\", \"WTS Paradigm\"]\n",
      "    },\n",
      "    \"soft_skills\": [\"communication\", \"teamwork\", \"independent judgment\", \"problem-solving\"],\n",
      "    \"level_of_exp\": \"mid-level\",\n",
      "    \"education\": [\"Bachelor’s Degree in related field\"]\n",
      "  },\n",
      "  \"resume_evaluation\": {\n",
      "    \"original_scores\": {\n",
      "      \"technical_skills\": 20,\n",
      "      \"soft_skills\": 50,\n",
      "      \"experience\": 50,\n",
      "      \"education\": 0\n",
      "    },\n",
      "    \"missing_skills\": [\"ASP.NET\", \"SQL\", \"VB.NET\", \"C#\", SSRS, \"ERP systems\", \"WTS Paradigm\"]\n",
      "  },\n",
      "  \"deeper_analysis\": {\n",
      "    \"inferred_experiences\": {\n",
      "      \"SQL\": \"inferred from working on Excel Macro and MIS\",\n",
      "      \"VBA\": \"inferred from working on Excel Macro\"\n",
      "    }\n",
      "  },\n",
      "  \"recalibrated_scores\": {\n",
      "    \"technical_skills\": 40,\n",
      "    \"soft_skills\": 50,\n",
      "    \"experience\": 50,\n",
      "    \"education\": 0\n",
      "  },\n",
      "  \"assessment\": {\n",
      "    \"suitability\": \"no\",\n",
      "    \"strengths\": [\"Experience with Excel Macros\", \"Experience in MIS reporting\"],\n",
      "    \"potential_concerns\": [\"Lack of direct experience with ASP.NET, VB.NET, C#\", \"No Bachelor's Degree in a related field\"],\n",
      "    \"missing_skills\": [\"ASP.NET\", \"VB.NET\", \"C#\", \"SSRS\", \"ERP systems\", \"WTS Paradigm\"],\n",
      "    \"reasons\": [\n",
      "      \"The candidate lacks essential technical skills such as ASP.NET, VB.NET, and C#.\",\n",
      "      \"The candidate does not have a Bachelor's Degree in a related field, which is a requirement.\",\n",
      "      \"The candidate's experience is more aligned with business analysis and MIS reporting rather than software development.\"\n",
      "    ]\n",
      "  },\n",
      "  \"feedback\": {\n",
      "    \"significant_mismatches\": [\n",
      "      \"The candidate should consider gaining experience in ASP.NET, VB.NET, and C# to align better with the job requirements.\",\n",
      "      \"Pursuing a Bachelor's Degree in a related field or obtaining relevant certifications could improve the candidate's qualifications.\"\n",
      "    ]\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"evaluation\": \"The candidate is not a suitable match for the Programmer/Software Developer role due to significant gaps in essential technical skills and educational qualifications. The candidate's experience is more aligned with business analysis rather than software development, which is the primary focus of the job description.\",\n",
      "    \"decision\": \"no\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "All models failed for job_id: ca0539ba-835b-48fb-bed0-705d6668c372, cv_id: 508190c8-d2e1-4d77-b19d-11a1aaa07ebd.\n"
     ]
    }
   ],
   "source": [
    "process_all_pairs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = [mod for mod, _ in model_tuples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "log_file_dir = Path(output_dir, \"evaluation_log.txt\")\n",
    "\n",
    "with open(log_file_dir, \"r\") as file:\n",
    "    log_content = file.read()\n",
    "\n",
    "error_entries = re.findall(f'ERROR - Error with .* for job_id:.*, cv_id:.*(?=\\.)', log_content)\n",
    "uuid_pattern =  r\"^[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89abAB][0-9a-fA-F]{3}-[0-9a-fA-F]{12}$\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR - Error with gpt4o for job_id: ca0539ba-835b-48fb-bed0-705d6668c372, cv_id: 508190c8-d2e1-4d77-b19d-11a1aaa07ebd\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entry = error_entries[0]\n",
    "print(entry)\n",
    "\n",
    "job_id = re.search(f'job_id: ({uuid_pattern})', entry)\n",
    "cv_id = re.search(f'cv_id: ({uuid_pattern})', entry)\n",
    "\n",
    "job_id, cv_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pattern = r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - ERROR -'\n",
    "info_pattern =r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2} - INFO -' \n",
    "sections = re.split(error_pattern, log_content, flags=re.MULTILINE)\n",
    "sections = [item.strip() for item in sections if item.strip().startswith(\"Error with \")]\n",
    "sections = [re.split(info_pattern, sec)[0] for sec in sections]\n",
    "json_strs = [re.search(r'(\\{.*\\})', sec, re.DOTALL).group(1) for sec in sections]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\n  \"job_description_analysis\": {\\n    \"technical_skills\": {\\n      \"essential\": [\"ASP.NET\", \"SQL\", \"VBA\", \"VB.NET\", \"C#\"],\\n      \"advantageous\": [\"SSRS\", \"ERP systems\", \"WTS Paradigm\"]\\n    },\\n    \"soft_skills\": [\"communication\", \"teamwork\", \"independent judgment\", \"problem-solving\"],\\n    \"level_of_exp\": \"mid-level\",\\n    \"education\": [\"Bachelor’s Degree in related field\"]\\n  },\\n  \"resume_evaluation\": {\\n    \"original_scores\": {\\n      \"technical_skills\": 20,\\n      \"soft_skills\": 50,\\n      \"experience\": 50,\\n      \"education\": 0\\n    },\\n    \"missing_skills\": [\"ASP.NET\", \"SQL\", \"VB.NET\", \"C#\", SSRS, \"ERP systems\", \"WTS Paradigm\"]\\n  },\\n  \"deeper_analysis\": {\\n    \"inferred_experiences\": {\\n      \"SQL\": \"inferred from working on Excel Macro and MIS\",\\n      \"VBA\": \"inferred from working on Excel Macro\"\\n    }\\n  },\\n  \"recalibrated_scores\": {\\n    \"technical_skills\": 40,\\n    \"soft_skills\": 50,\\n    \"experience\": 50,\\n    \"education\": 0\\n  },\\n  \"assessment\": {\\n    \"suitability\": \"no\",\\n    \"strengths\": [\"Experience with Excel Macros\", \"Experience in MIS reporting\"],\\n    \"potential_concerns\": [\"Lack of direct experience with ASP.NET, VB.NET, C#\", \"No Bachelor\\'s Degree in a related field\"],\\n    \"missing_skills\": [\"ASP.NET\", \"VB.NET\", \"C#\", \"SSRS\", \"ERP systems\", \"WTS Paradigm\"],\\n    \"reasons\": [\\n      \"The candidate lacks essential technical skills such as ASP.NET, VB.NET, and C#.\",\\n      \"The candidate does not have a Bachelor\\'s Degree in a related field, which is a requirement.\",\\n      \"The candidate\\'s experience is more aligned with business analysis and MIS reporting rather than software development.\"\\n    ]\\n  },\\n  \"feedback\": {\\n    \"significant_mismatches\": [\\n      \"The candidate should consider gaining experience in ASP.NET, VB.NET, and C# to align better with the job requirements.\",\\n      \"Pursuing a Bachelor\\'s Degree in a related field or obtaining relevant certifications could improve the candidate\\'s qualifications.\"\\n    ]\\n  },\\n  \"summary\": {\\n    \"evaluation\": \"The candidate is not a suitable match for the Programmer/Software Developer role due to significant gaps in essential technical skills and educational qualifications. The candidate\\'s experience is more aligned with business analysis rather than software development, which is the primary focus of the job description.\",\\n    \"decision\": \"no\"\\n  }\\n}']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_strs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON: Expecting value: line 18 column 58 (char 592)\n",
      "{\n",
      "  \"job_description_analysis\": {\n",
      "    \"technical_skills\": {\n",
      "      \"essential\": [\"ASP.NET\", \"SQL\", \"VBA\", \"VB.NET\", \"C#\"],\n",
      "      \"advantageous\": [\"SSRS\", \"ERP systems\", \"WTS Paradigm\"]\n",
      "    },\n",
      "    \"soft_skills\": [\"communication\", \"teamwork\", \"independent judgment\", \"problem-solving\"],\n",
      "    \"level_of_exp\": \"mid-level\",\n",
      "    \"education\": [\"Bachelor’s Degree in related field\"]\n",
      "  },\n",
      "  \"resume_evaluation\": {\n",
      "    \"original_scores\": {\n",
      "      \"technical_skills\": 20,\n",
      "      \"soft_skills\": 50,\n",
      "      \"experience\": 50,\n",
      "      \"education\": 0\n",
      "    },\n",
      "    \"missing_skills\": [\"ASP.NET\", \"SQL\", \"VB.NET\", \"C#\", SSRS, \"ERP systems\", \"WTS Paradigm\"]\n",
      "  },\n",
      "  \"deeper_analysis\": {\n",
      "    \"inferred_experiences\": {\n",
      "      \"SQL\": \"inferred from working on Excel Macro and MIS\",\n",
      "      \"VBA\": \"inferred from working on Excel Macro\"\n",
      "    }\n",
      "  },\n",
      "  \"recalibrated_scores\": {\n",
      "    \"technical_skills\": 40,\n",
      "    \"soft_skills\": 50,\n",
      "    \"experience\": 50,\n",
      "    \"education\": 0\n",
      "  },\n",
      "  \"assessment\": {\n",
      "    \"suitability\": \"no\",\n",
      "    \"strengths\": [\"Experience with Excel Macros\", \"Experience in MIS reporting\"],\n",
      "    \"potential_concerns\": [\"Lack of direct experience with ASP.NET, VB.NET, C#\", \"No Bachelor's Degree in a related field\"],\n",
      "    \"missing_skills\": [\"ASP.NET\", \"VB.NET\", \"C#\", \"SSRS\", \"ERP systems\", \"WTS Paradigm\"],\n",
      "    \"reasons\": [\n",
      "      \"The candidate lacks essential technical skills such as ASP.NET, VB.NET, and C#.\",\n",
      "      \"The candidate does not have a Bachelor's Degree in a related field, which is a requirement.\",\n",
      "      \"The candidate's experience is more aligned with business analysis and MIS reporting rather than software development.\"\n",
      "    ]\n",
      "  },\n",
      "  \"feedback\": {\n",
      "    \"significant_mismatches\": [\n",
      "      \"The candidate should consider gaining experience in ASP.NET, VB.NET, and C# to align better with the job requirements.\",\n",
      "      \"Pursuing a Bachelor's Degree in a related field or obtaining relevant certifications could improve the candidate's qualifications.\"\n",
      "    ]\n",
      "  },\n",
      "  \"summary\": {\n",
      "    \"evaluation\": \"The candidate is not a suitable match for the Programmer/Software Developer role due to significant gaps in essential technical skills and educational qualifications. The candidate's experience is more aligned with business analysis rather than software development, which is the primary focus of the job description.\",\n",
      "    \"decision\": \"no\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for json_str in json_strs:\n",
    "  try:\n",
    "    result = json.loads(json_str)\n",
    "    print(result) \n",
    "  except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SSRS, \"E'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str[592:600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original error: Expecting value: line 18 column 58 (char 592)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jsonlint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 107\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     53\u001b[0m json_with_errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124m  \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob_description_analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtechnical_skills\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m{\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124m  }\u001b[39m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m--> 107\u001b[0m fixed_json, error \u001b[38;5;241m=\u001b[39m \u001b[43mfix_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_with_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[54], line 15\u001b[0m, in \u001b[0;36mfix_json\u001b[0;34m(json_string)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Use jsonlint to get more detailed error information\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mjsonlint\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-c\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     18\u001b[0m     error_output \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mstderr\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm/lib/python3.11/subprocess.py:548\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    546\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm/lib/python3.11/subprocess.py:1024\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1021\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1022\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1025\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llm/lib/python3.11/subprocess.py:1917\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1915\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1916\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1918\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'jsonlint'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "def fix_json(json_string):\n",
    "    # Try to parse the JSON first\n",
    "    try:\n",
    "        json.loads(json_string)\n",
    "        return json_string, None  # If it's valid, return as is\n",
    "    except json.JSONDecodeError as e:\n",
    "        error_message = str(e)\n",
    "        print(f\"Original error: {error_message}\")\n",
    "\n",
    "    # Use jsonlint to get more detailed error information\n",
    "    result = subprocess.run(['jsonlint', '-c'], input=json_string, text=True, capture_output=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        error_output = result.stderr\n",
    "        \n",
    "        # Extract line and column information\n",
    "        match = re.search(r'line (\\d+), col (\\d+)', error_output)\n",
    "        if match:\n",
    "            line, col = map(int, match.groups())\n",
    "            \n",
    "            # Split the JSON string into lines\n",
    "            lines = json_string.split('\\n')\n",
    "            \n",
    "            # Fix common errors\n",
    "            if 'Expecting' in error_output:\n",
    "                if 'value' in error_output:\n",
    "                    # Missing value after colon\n",
    "                    lines[line-1] = lines[line-1][:col-1] + ' null' + lines[line-1][col-1:]\n",
    "                elif 'property name' in error_output:\n",
    "                    # Missing property name or extra comma\n",
    "                    if lines[line-1].strip().endswith(','):\n",
    "                        lines[line-1] = lines[line-1].rstrip()[:-1]  # Remove trailing comma\n",
    "                    else:\n",
    "                        lines[line-1] = lines[line-1][:col-1] + '\"missing_property\": null' + lines[line-1][col-1:]\n",
    "            elif 'Unexpected' in error_output:\n",
    "                if '}' in error_output or ']' in error_output:\n",
    "                    # Extra comma before closing bracket\n",
    "                    lines[line-1] = lines[line-1][:col-1] + lines[line-1][col:]\n",
    "            \n",
    "            # Join the lines back together\n",
    "            fixed_json = '\\n'.join(lines)\n",
    "            \n",
    "            # Recursively try to fix any remaining errors\n",
    "            return fix_json(fixed_json)\n",
    "    \n",
    "    return json_string, \"Failed to fix JSON errors\"\n",
    "\n",
    "# Example usage\n",
    "json_with_errors = '''{\n",
    "  \"job_description_analysis\": {\n",
    "    \"technical_skills\": {\n",
    "      \"essential\": [\"ASP.NET\", \"SQL\", \"VBA\", \"VB.NET\", \"C#\"],\n",
    "      \"advantageous\": [\"SSRS\", \"ERP systems\", \"WTS Paradigm\"]\n",
    "    },\n",
    "    \"soft_skills\": [\"communication\", \"teamwork\", \"independent judgment\", \"problem-solving\"],\n",
    "    \"level_of_exp\": \"mid-level\",\n",
    "    \"education\": [\"Bachelor's Degree in related field\"]\n",
    "  },\n",
    "  \"resume_evaluation\": {\n",
    "    \"original_scores\": {\n",
    "      \"technical_skills\": 20,\n",
    "      \"soft_skills\": 50,\n",
    "      \"experience\": 50,\n",
    "      \"education\": 0\n",
    "    },\n",
    "    \"missing_skills\": [\"ASP.NET\", \"SQL\", \"VB.NET\", \"C#\", SSRS, \"ERP systems\", \"WTS Paradigm\"]\n",
    "  },\n",
    "  \"deeper_analysis\": {\n",
    "    \"inferred_experiences\": {\n",
    "      \"SQL\": \"inferred from working on Excel Macro and MIS\",\n",
    "      \"VBA\": \"inferred from working on Excel Macro\"\n",
    "    }\n",
    "  },\n",
    "  \"recalibrated_scores\": {\n",
    "    \"technical_skills\": 40,\n",
    "    \"soft_skills\": 50,\n",
    "    \"experience\": 50,\n",
    "    \"education\": 0\n",
    "  },\n",
    "  \"assessment\": {\n",
    "    \"suitability\": \"no\",\n",
    "    \"strengths\": [\"Experience with Excel Macros\", \"Experience in MIS reporting\"],\n",
    "    \"potential_concerns\": [\"Lack of direct experience with ASP.NET, VB.NET, C#\", \"No Bachelor's Degree in a related field\"],\n",
    "    \"missing_skills\": [\"ASP.NET\", \"VB.NET\", \"C#\", \"SSRS\", \"ERP systems\", \"WTS Paradigm\"],\n",
    "    \"reasons\": [\n",
    "      \"The candidate lacks essential technical skills such as ASP.NET, VB.NET, and C#.\",\n",
    "      \"The candidate does not have a Bachelor's Degree in a related field, which is a requirement.\",\n",
    "      \"The candidate's experience is more aligned with business analysis and MIS reporting rather than software development.\"\n",
    "    ]\n",
    "  },\n",
    "  \"feedback\": {\n",
    "    \"significant_mismatches\": [\n",
    "      \"The candidate should consider gaining experience in ASP.NET, VB.NET, and C# to align better with the job requirements.\",\n",
    "      \"Pursuing a Bachelor's Degree in a related field or obtaining relevant certifications could improve the candidate's qualifications.\"\n",
    "    ]\n",
    "  },\n",
    "  \"summary\": {\n",
    "    \"evaluation\": \"The candidate is not a suitable match for the Programmer/Software Developer role due to significant gaps in essential technical skills and educational qualifications. The candidate's experience is more aligned with business analysis rather than software development, which is the primary focus of the job description.\",\n",
    "    \"decision\": \"no\"\n",
    "  }\n",
    "}'''\n",
    "\n",
    "fixed_json, error = fix_json(json_with_errors)\n",
    "if error:\n",
    "    print(f\"Error: {error}\")\n",
    "else:\n",
    "    print(\"Fixed JSON:\")\n",
    "    print(fixed_json)\n",
    "\n",
    "# Verify the fixed JSON\n",
    "try:\n",
    "    json.loads(fixed_json)\n",
    "    print(\"\\nJSON is now valid!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nJSON is still invalid. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlint\n",
      "  Downloading jsonlint-0.1-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Downloading jsonlint-0.1-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.7/103.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: jsonlint\n",
      "Successfully installed jsonlint-0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
